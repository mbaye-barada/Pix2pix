# -*- coding: utf-8 -*-
"""Copie de pix2pix_sat_imgs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gedyO3q39-3CGiaQPvOoKASf5_SQZKjP

#Image-to-Image Translation with Conditional Adversarial Nets

In this notebook we impleted Pix2Pix by , an image-to-image translation using Conditional Adversarial Nets. The achitecture uses the conditinal version of GANs with the following specifities:

-The generator uses Unet architecture;

-And the Discrimitor implementes a Patch version called PatchGAN
"""

# Import of the required librairies
import torch
from torch import nn
from torchvision import transforms
from torchvision.utils import make_grid
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import torch.nn.functional as F
torch.manual_seed(0)



"""# Generator

The Generator uses Unet architecture:
"""

class EncoderUnit(nn.Module):
    
    def __init__(self, input_channels, use_dropout=False, use_bn=True):
        super(EncoderUnit, self).__init__()
        self.conv_layer1 = nn.Conv2d(input_channels, input_channels * 2,kernel_size=3,padding=1)
        self.conv_layer2 = nn.Conv2d(input_channels * 2, input_channels * 2, kernel_size=3, padding=1)
        self.act_function= nn.LeakyReLU(0.2)
        self.pooling_layer = nn.MaxPool2d(kernel_size=2, stride=2)
        if use_bn:
            self.bn = nn.BatchNorm2d(input_channels * 2)
        self.use_bn = use_bn
        if use_dropout:
            self.dropout = nn.Dropout()
        self.use_dropout = use_dropout

    def forward(self, input_fm):
        out = self.conv_layer1(input_fm)
        if self.use_bn:
            out = self.bn(out)
        if self.use_dropout:
            out = self.dropout(out)
        out = self.act_function(out)
        out = self.conv_layer2(out)
        if self.use_bn:
            out = self.bn(out)
        if self.use_dropout:
            out = self.dropout(out)
        out = self.act_function(out)
        out = self.pooling_layer(out)
        return out

x = torch.randn((1,3,256,256))
eu = EncoderUnit(3)
y = eu(x)
y.shape

class DecoderUnit(nn.Module):
    
    def __init__(self, input_channels, use_dropout=False, use_bn=True):
        super(DecoderUnit, self).__init__()
        self.upsampling_layer = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        self.conv_layer1 = nn.Conv2d(input_channels, input_channels // 2, kernel_size=2)
        self.conv_layer2 = nn.Conv2d(input_channels, input_channels // 2, kernel_size=3, padding=1)
        self.conv_layer3 = nn.Conv2d(input_channels // 2, input_channels // 2, kernel_size=2, padding=1)
        if use_bn:
            self.bn = nn.BatchNorm2d(input_channels // 2)
        self.use_bn = use_bn
        self.act_function = nn.ReLU()
        if use_dropout:
            self.dropout = nn.Dropout()
        self.use_dropout = use_dropout

    

    def forward(self, input_fm, input_fm_skip_con):
      
        input_fm = self.upsampling_layer(input_fm)
        input_fm = self.conv_layer1(input_fm)
        cropped_input_fm = self.crop_image(input_fm_skip_con, input_fm.shape)
        out_fm = torch.cat([input_fm, cropped_input_fm ], axis=1)
        decoded_fm = self.conv_layer2(out_fm)
        if self.use_bn:
            decoded_fm = self.bn(decoded_fm)
        if self.use_dropout:
            decoded_fm = self.dropout(decoded_fm)
        decoded_fm = self.act_function(decoded_fm)
        decoded_fm = self.conv_layer3(decoded_fm)
        if self.use_bn:
            decoded_fm = self.bn(decoded_fm)
        if self.use_dropout:
            decoded_fm = self.dropout(decoded_fm)
        decoded_fm = self.act_function(decoded_fm)
        return decoded_fm

    def crop_image(self, image, target_shape):
    
      center_height = image.shape[2] // 2
      center_width = image.shape[3] // 2
      top_left = center_height - round(target_shape[2] / 2)
      top_right = top_left + target_shape[2]
      bottom_left = center_width - round(target_shape[3] / 2)
      bottom_right = bottom_left + target_shape[3]
      self.new_image = image[:, :, top_left:top_right, bottom_left:bottom_right]
      return self.new_image

class UnetGenerator(nn.Module):
    
    def __init__(self, input_channels, output_channels, hidden_channels=32):
        super(UnetGenerator, self).__init__()

        
        self.unet_first_layer = nn.Sequential(
            nn.Conv2d(input_channels, hidden_channels, 1),
            nn.LeakyReLU(0.2))
        #self.unet_first_layer = FeatureMapBlock(input_channels, hidden_channels)
        self.encoder1 = EncoderUnit(hidden_channels, use_dropout=True)
        self.encoder2 = EncoderUnit(hidden_channels * 2, use_dropout=True)
        self.encoder3 = EncoderUnit(hidden_channels * 4, use_dropout=True)
        self.encoder4 = EncoderUnit(hidden_channels * 8)
        self.encoder5 = EncoderUnit(hidden_channels * 16)
        self.encoder6 = EncoderUnit(hidden_channels * 32)
        self.decoder0 = DecoderUnit(hidden_channels * 64)
        self.decoder1 = DecoderUnit(hidden_channels * 32)
        self.decoder2 = DecoderUnit(hidden_channels * 16)
        self.decoder3 = DecoderUnit(hidden_channels * 8)
        self.decoder4 = DecoderUnit(hidden_channels * 4)
        self.decoder5 = DecoderUnit(hidden_channels * 2)

        self.unet_last_layer = nn.Sequential(
            nn.Conv2d(hidden_channels, output_channels, 1),
            nn.Tanh())
        #self.sigmoid = torch.nn.Sigmoid()

    def forward(self, real_input):

        x0 = self.unet_first_layer(real_input)
        
        x1 = self.encoder1(x0)
        x2 = self.encoder2(x1)
        x3 = self.encoder3(x2)
        x4 = self.encoder4(x3)
        x5 = self.encoder5(x4)
        x6 = self.encoder6(x5)
        x7 = self.decoder0(x6, x5)
        x8 = self.decoder1(x7, x4)
        x9 = self.decoder2(x8, x3)
        x10 = self.decoder3(x9, x2)
        x11 = self.decoder4(x10, x1)
        x12 = self.decoder5(x11, x0)
        gen_image = self.unet_last_layer(x12)
        return gen_image
        #return self.sigmoid(gen_image)

x = torch.randn(1,3,256,256)
ge =  UnetGenerator(3,3)
a = ge(x)
a.shape

"""#Create the Discriminator"""

class PatchGanDis(nn.Module):
    
    def __init__(self, input_channels, hidden_channels=8):
        super(PatchGanDis, self).__init__()

        self.patchGan_first_layer = nn.Sequential(
            nn.Conv2d(input_channels, hidden_channels,1),
            nn.LeakyReLU(0.2))
        
        self.patchGan_layer1 = EncoderUnit(hidden_channels, use_bn=False)
        self.patchGan_layer2 = EncoderUnit(hidden_channels * 2)
        self.patchGan_layer3 = EncoderUnit(hidden_channels * 4)
        self.patchGan_layer4 = EncoderUnit(hidden_channels * 8)
        self.patchGan_final_layer = nn.Conv2d(hidden_channels * 16, 1, kernel_size=1)
  

    def forward(self, gen_img, real_output):

        x = torch.cat((gen_img, real_output), axis=1)
        x0 = self.patchGan_first_layer(x)
        x1 = self.patchGan_layer1(x0)
        x2 = self.patchGan_layer2(x1)
        x3 = self.patchGan_layer3(x2)
        x4 = self.patchGan_layer4(x3)
        realness_probas_marix = self.patchGan_final_layer(x4)
        return realness_probas_marix

"""# upload Dataset"""

from google.colab import drive
drive.mount('/content/gdrive')

# New parameters
lCGAN_criterion = nn.BCEWithLogitsLoss() 
pix_dist_criterion = nn.L1Loss() 
lambda_pix_dist = 200

n_epochs = 30
input_img_channels = 3
real_img_channels = 3
display_step = 200
batch_size = 4
lr = 0.0002
target_shape = 256
device = 'cuda'

dataset_path  = '/content/drive/MyDrive/Colab Notebooks/deep_learinng_projects/computer_vision/dataset/map_dataset/maps'

transform = transforms.Compose([
    transforms.ToTensor(),
])

import torchvision
dataset = torchvision.datasets.ImageFolder(dataset_path, transform=transform)



gen = UnetGenerator(input_img_channels, real_img_channels).to(device)
gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)
disc = PatchGanDis(input_img_channels + real_img_channels).to(device)
disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)

def weights_init(m):
    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):
        torch.nn.init.normal_(m.weight, 0.0, 0.02)
    if isinstance(m, nn.BatchNorm2d):
        torch.nn.init.normal_(m.weight, 0.0, 0.02)
        torch.nn.init.constant_(m.bias, 0)


gen = gen.apply(weights_init)
disc = disc.apply(weights_init)

def plot_images(image_tensor, num_images=25, size=(1, 28, 28)):
  
    image_shifted = image_tensor
    image_unflat = image_shifted.detach().cpu().view(-1, *size)
    image_grid = make_grid(image_unflat[:num_images], nrow=5)
    plt.imshow(image_grid.permute(1, 2, 0).squeeze())
    plt.show()

from skimage import color
import numpy as np


mean_generator_loss = 0
mean_discriminator_loss = 0
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
cur_step = 0

discriminator_losses = []
generator_losses = []

for epoch in range(n_epochs):
    # Dataloader returns the batches
    for image, _ in dataloader:
        image_width = image.shape[3]
        condition = image[:, :, :, :image_width // 2]
        condition = nn.functional.interpolate(condition, size=target_shape)
        real = image[:, :, :, image_width // 2:]
        real = nn.functional.interpolate(real, size=target_shape)
        cur_batch_size = len(condition)
        condition = condition.to(device)
        real = real.to(device)

        ### Update discriminator ###
        disc_opt.zero_grad() 
        with torch.no_grad():
            fake = gen(condition)
        disc_fake_hat = disc(fake.detach(), condition) # Detach generator
        disc_fake_loss = lCGAN_criterion(disc_fake_hat, torch.zeros_like(disc_fake_hat))
        disc_real_hat = disc(real, condition)
        disc_real_loss = lCGAN_criterion(disc_real_hat, torch.ones_like(disc_real_hat))
        disc_loss = (disc_fake_loss + disc_real_loss) / 2
        disc_loss.backward(retain_graph=True) 
        disc_opt.step()

        ### Update generator ###
        gen_opt.zero_grad()
        fake = gen(condition)
        disc_fake_bar = disc(fake, condition)
        gen_adv_loss = lCGAN_criterion(disc_fake_bar, torch.ones_like(disc_fake_bar))
        gen_rec_loss = pix_dist_criterion(real, fake)
        gen_loss_tot = gen_adv_loss + lambda_pix_dist * gen_rec_loss
        
        gen_loss_tot.backward() 
        gen_opt.step() 

        
        mean_discriminator_loss += disc_loss.item() / display_step
        mean_generator_loss += gen_loss_tot.item() / display_step

        discriminator_losses.append(mean_discriminator_loss)
        generator_losses.append(mean_discriminator_loss)

        #Visualization 
        if cur_step % display_step == 0:
            if cur_step > 0:
                print(f"Epoch {epoch}: Step {cur_step}: Generator (U-Net) loss: {mean_generator_loss}, Discriminator loss: {mean_discriminator_loss}")
            else:
                print("Pretrained initial state")
            plot_images(condition, size=(input_img_channels, target_shape, target_shape))
            plot_images(real, size=(input_img_channels, target_shape, target_shape))
            plot_images(fake, size=(input_img_channels, target_shape, target_shape))
            mean_generator_loss = 0
            mean_discriminator_loss = 0
        cur_step += 1













